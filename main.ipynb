{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torchfile\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBase(nn.Sequential):\n",
    "    def __init__(self, fn, *args):\n",
    "        super(LBase, self).__init__(*args)\n",
    "        self.lambda_func = fn\n",
    "\n",
    "    def forward_prepare(self, x):\n",
    "        output = []\n",
    "\n",
    "        for module in self._modules.values():\n",
    "            output.append(module(x))\n",
    "\n",
    "        return output if output else x\n",
    "\n",
    "\n",
    "class LForward(LBase):\n",
    "    def forward(self, x):\n",
    "        return self.lambda_func(self.forward_prepare(x))\n",
    "\n",
    "\n",
    "class LMap(LBase):\n",
    "    def forward(self, x):\n",
    "        return list(map(self.lambda_func, self.forward_prepare(x)))\n",
    "\n",
    "\n",
    "class LReduce(LBase):\n",
    "    def forward(self, x):\n",
    "        return reduce(self.lambda_func, self.forward_prepare(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Sequential(\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 32, (3, 3), (2, 2), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 nn.Sequential(\n",
    "                          nn.AvgPool2d((1, 1), (2, 2)),\n",
    "                          LReduce(lambda x, y, dim=1: torch.cat((x, y), dim),\n",
    "                                  LForward(lambda x: x),\n",
    "                                  LForward(lambda x: x * 0),\n",
    "                                  ),\n",
    "                      ),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 64, (3, 3), (2, 2), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 nn.Sequential(\n",
    "                          nn.AvgPool2d((1, 1), (2, 2)),\n",
    "                          LReduce(lambda x, y, dim=1: torch.cat((x, y), dim),\n",
    "                                  LForward(lambda x: x),\n",
    "                                  LForward(lambda x: x * 0),\n",
    "                                  ),\n",
    "                      ),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "    ),\n",
    "    nn.AvgPool2d((8, 8), (1, 1)),\n",
    "    LForward(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Sequential(LForward(lambda x: x.view(1, -1) if 1 == len(x.size()) else x), nn.Linear(64, 10)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA_F = 0.5\n",
    "CHECK_STEP = 10000\n",
    "D_PATH = 'MNIST.t7'\n",
    "modelPath = 'm.pt'\n",
    "GAMMA_T = 0.0001\n",
    "L_RATE = 0.001\n",
    "MAX_ITER = 10000\n",
    "TR = False\n",
    "PRINT_STEP = 1000\n",
    "B_SIZE = 100\n",
    "FILTERS = 15\n",
    "ROUNDS = 5\n",
    "CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write(print_arr):\n",
    "    for v in print_arr: sys.stdout.write(str(v) + '\\t')\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def _transform(X):\n",
    "    tmp = np.zeros((np.shape(X)[0], 3, 38, 38))\n",
    "    tmp[:, :, 2:34, 2:34] = X\n",
    "\n",
    "    for i in range(np.shape(X)[0]):\n",
    "        r1 = np.random.randint(4)\n",
    "        r2 = np.random.randint(4)\n",
    "        X[i] = tmp[i, :, r1: r1 + 32, r2: r2 + 32]\n",
    "\n",
    "        if np.random.uniform() > .5:\n",
    "            X[i] = X[i, :, :, ::-1]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def _performance(net, X, y, n):\n",
    "    acc = 0.\n",
    "    model.eval()\n",
    "    out = np.zeros((X.shape[0], 10))\n",
    "\n",
    "    for batch in range(int(X.shape[0] / B_SIZE)):\n",
    "        start = batch * B_SIZE\n",
    "        stop = (batch + 1) * B_SIZE - 1\n",
    "        ints = np.linspace(start, stop, B_SIZE).astype(int)\n",
    "        data = Variable(torch.from_numpy(X[ints])).float()\n",
    "\n",
    "        for i in range(n):\n",
    "            data = BLOCKS[i](data)\n",
    "\n",
    "        output = net(data)\n",
    "        acc += np.mean(torch.max(output, 1)[1].cpu().data.numpy() == y[ints])\n",
    "        out[ints] = output.cpu().data.numpy()\n",
    "\n",
    "    acc /= (X.shape[0] / B_SIZE)\n",
    "    model.train()\n",
    "    return acc, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKS = {0: nn.Sequential(model[0], model[1], model[2])}\n",
    "\n",
    "for i in range(8):\n",
    "    BLOCKS[1 + i] = model[3][i]\n",
    "\n",
    "for i in range(8):\n",
    "    BLOCKS[9 + i] = model[4][i]\n",
    "\n",
    "for i in range(8):\n",
    "    BLOCKS[17 + i] = model[5][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./svhn/train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "data = torchvision.datasets.SVHN(\"./svhn\", download=True)\n",
    "\n",
    "X_train = data.data[:60000]\n",
    "y_train = data.labels[:60000]\n",
    "X_test = data.data[60000:]\n",
    "y_test = data.labels[60000:]\n",
    "\n",
    "cut = int(np.shape(X_train)[0] / B_SIZE * B_SIZE)\n",
    "\n",
    "train_size = cut\n",
    "X_train = X_train[:cut]\n",
    "y_train = y_train[:cut]\n",
    "\n",
    "cut = int(np.shape(X_test)[0] / B_SIZE * B_SIZE)\n",
    "\n",
    "test_size = cut\n",
    "X_test = X_test[:test_size]\n",
    "y_test = y_test[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pikalm/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: DeprecationWarning: This function is deprecated. Please call randint(1, 59999 + 1) instead\n",
      "/home/pikalm/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:72: DeprecationWarning: This function is deprecated. Please call randint(1, 13256 + 1) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\t0\tbatch\t1000\ttrain_acc\t0.2601699999999999\ttest_acc\t0.2522900000000005\t\n",
      "n\t0\tbatch\t2000\ttrain_acc\t0.47547999999999935\ttest_acc\t0.45938000000000007\t\n",
      "n\t0\tbatch\t3000\ttrain_acc\t0.5665199999999996\ttest_acc\t0.5540099999999998\t\n",
      "n\t0\tbatch\t4000\ttrain_acc\t0.642590000000001\ttest_acc\t0.6291400000000011\t\n",
      "n\t0\tbatch\t5000\ttrain_acc\t0.7017799999999998\ttest_acc\t0.6887399999999999\t\n",
      "n\t0\tbatch\t6000\ttrain_acc\t0.7331199999999995\ttest_acc\t0.717440000000001\t\n",
      "n\t0\tbatch\t7000\ttrain_acc\t0.7554899999999998\ttest_acc\t0.7378699999999992\t\n",
      "n\t0\tbatch\t8000\ttrain_acc\t0.7777499999999989\ttest_acc\t0.7648499999999985\t\n",
      "n\t0\tbatch\t9000\ttrain_acc\t0.7918999999999977\ttest_acc\t0.7762799999999985\t\n",
      "n\t0\tbatch\t10000\ttrain_acc\t0.7984799999999992\ttest_acc\t0.7872499999999986\t\n",
      "-0.5659286583433925\t0.09896244363007538\t0.5\t\n",
      "a_{t+1}:\t0.09928742043677384\tgamma:\t-0.5659286583433925\t\n",
      "t\t5\tn_batches:\t10000\ttest_acc:\t0.8076487893188502\t\n",
      "n\t1\tbatch\t11000\ttrain_acc\t0.8022599999999999\ttest_acc\t0.7973499999999987\t\n",
      "n\t1\tbatch\t12000\ttrain_acc\t0.8257700000000006\ttest_acc\t0.8197799999999997\t\n",
      "n\t1\tbatch\t13000\ttrain_acc\t0.834930000000002\ttest_acc\t0.8265400000000017\t\n",
      "n\t1\tbatch\t14000\ttrain_acc\t0.8386100000000016\ttest_acc\t0.8358500000000005\t\n",
      "n\t1\tbatch\t15000\ttrain_acc\t0.846800000000001\ttest_acc\t0.84403\t\n",
      "n\t1\tbatch\t16000\ttrain_acc\t0.8502100000000014\ttest_acc\t0.8462900000000017\t\n",
      "n\t1\tbatch\t17000\ttrain_acc\t0.8516900000000008\ttest_acc\t0.8483900000000012\t\n",
      "n\t1\tbatch\t18000\ttrain_acc\t0.8595300000000015\ttest_acc\t0.8583299999999994\t\n",
      "n\t1\tbatch\t19000\ttrain_acc\t0.8645700000000011\ttest_acc\t0.8577800000000018\t\n",
      "n\t1\tbatch\t20000\ttrain_acc\t0.8662799999999994\ttest_acc\t0.8609600000000006\t\n",
      "-0.016804615813100974\t0.09753940635236351\t0.09896244363007538\t\n",
      "a_{t+1}:\t0.09785051207977348\tgamma:\t-0.016804615813100974\t\n",
      "t\t5\tn_batches:\t10000\ttest_acc:\t0.8596213321264237\t\n",
      "n\t2\tbatch\t21000\ttrain_acc\t0.8604799999999999\ttest_acc\t0.8565300000000005\t\n",
      "n\t2\tbatch\t22000\ttrain_acc\t0.8671100000000007\ttest_acc\t0.8646899999999998\t\n",
      "n\t2\tbatch\t23000\ttrain_acc\t0.8736899999999981\ttest_acc\t0.8691299999999993\t\n",
      "n\t2\tbatch\t24000\ttrain_acc\t0.8763799999999987\ttest_acc\t0.870799999999998\t\n",
      "n\t2\tbatch\t25000\ttrain_acc\t0.8787399999999977\ttest_acc\t0.8730800000000006\t\n",
      "n\t2\tbatch\t26000\ttrain_acc\t0.8820699999999986\ttest_acc\t0.8748200000000004\t\n",
      "n\t2\tbatch\t27000\ttrain_acc\t0.8834399999999978\ttest_acc\t0.8791199999999987\t\n",
      "n\t2\tbatch\t28000\ttrain_acc\t0.884769999999999\ttest_acc\t0.8805499999999985\t\n",
      "n\t2\tbatch\t29000\ttrain_acc\t0.8821399999999983\ttest_acc\t0.8771499999999989\t\n",
      "n\t2\tbatch\t30000\ttrain_acc\t0.8824399999999971\ttest_acc\t0.881529999999999\t\n",
      "-0.010882202226862819\t0.09693626838631868\t0.09753940635236351\t\n",
      "a_{t+1}:\t0.09724161687039858\tgamma:\t-0.010882202226862819\t\n",
      "t\t5\tn_batches:\t10000\ttest_acc:\t0.8790827487365167\t\n",
      "n\t3\tbatch\t31000\ttrain_acc\t0.8806299999999969\ttest_acc\t0.8752399999999986\t\n",
      "n\t3\tbatch\t32000\ttrain_acc\t0.8865299999999969\ttest_acc\t0.8802199999999968\t\n",
      "n\t3\tbatch\t33000\ttrain_acc\t0.8858999999999975\ttest_acc\t0.8795699999999984\t\n",
      "n\t3\tbatch\t34000\ttrain_acc\t0.886179999999997\ttest_acc\t0.8820599999999971\t\n",
      "n\t3\tbatch\t35000\ttrain_acc\t0.8860299999999969\ttest_acc\t0.8822299999999985\t\n",
      "n\t3\tbatch\t36000\ttrain_acc\t0.8890899999999979\ttest_acc\t0.8852599999999968\t\n",
      "n\t3\tbatch\t37000\ttrain_acc\t0.8950199999999957\ttest_acc\t0.8924099999999968\t\n",
      "n\t3\tbatch\t38000\ttrain_acc\t0.8894499999999977\ttest_acc\t0.8848799999999969\t\n",
      "n\t3\tbatch\t39000\ttrain_acc\t0.893559999999995\ttest_acc\t0.8883099999999973\t\n",
      "n\t3\tbatch\t40000\ttrain_acc\t0.8939999999999969\ttest_acc\t0.8901799999999968\t\n",
      "-0.010408047609960065\t0.09638117340914863\t0.09693626838631868\t\n",
      "a_{t+1}:\t0.09668128676549574\tgamma:\t-0.010408047609960065\t\n",
      "t\t5\tn_batches:\t10000\ttest_acc:\t0.8851927283699181\t\n",
      "n\t4\tbatch\t41000\ttrain_acc\t0.8912699999999969\ttest_acc\t0.8869599999999958\t\n",
      "n\t4\tbatch\t42000\ttrain_acc\t0.8940599999999959\ttest_acc\t0.8882299999999976\t\n",
      "n\t4\tbatch\t43000\ttrain_acc\t0.8967299999999965\ttest_acc\t0.8930399999999977\t\n",
      "n\t4\tbatch\t44000\ttrain_acc\t0.8913199999999954\ttest_acc\t0.8887799999999976\t\n",
      "n\t4\tbatch\t45000\ttrain_acc\t0.8973499999999963\ttest_acc\t0.8919999999999972\t\n",
      "n\t4\tbatch\t46000\ttrain_acc\t0.8971999999999968\ttest_acc\t0.8916699999999975\t\n",
      "n\t4\tbatch\t47000\ttrain_acc\t0.8977299999999957\ttest_acc\t0.8928699999999967\t\n",
      "n\t4\tbatch\t48000\ttrain_acc\t0.8992799999999963\ttest_acc\t0.895659999999996\t\n",
      "n\t4\tbatch\t49000\ttrain_acc\t0.8928099999999957\ttest_acc\t0.8874499999999971\t\n",
      "n\t4\tbatch\t50000\ttrain_acc\t0.8999599999999968\ttest_acc\t0.8946099999999962\t\n",
      "-0.010161009602030636\t0.0958490665520352\t0.09638117340914863\t\n",
      "a_{t+1}:\t0.09614421839517759\tgamma:\t-0.010161009602030636\t\n",
      "t\t5\tn_batches:\t10000\ttest_acc:\t0.8971863920947424\t\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "a_prev = 0.0\n",
    "a_curr = -1.0\n",
    "s = np.zeros((train_size, CLASSES))\n",
    "cost = np.zeros((train_size, CLASSES))\n",
    "Xoutput_previous = np.zeros((train_size, CLASSES))\n",
    "y_batch = np.zeros((B_SIZE))\n",
    "y_batch_t = np.zeros((B_SIZE))\n",
    "g_curr = 0\n",
    "g_prev = GAMMA_F\n",
    "iters = 0\n",
    "tries = 0\n",
    "\n",
    "for n in range(ROUNDS):\n",
    "    gamma = -1\n",
    "    Z = 0\n",
    "\n",
    "    for i in range(train_size):\n",
    "        l_sum = 0\n",
    "\n",
    "        for l in range(CLASSES):\n",
    "            if l != y_train[i]:\n",
    "                cost[i][l] = np.exp(s[i][l] - s[i][int(y_train[i])])\n",
    "                l_sum += cost[i][l]\n",
    "\n",
    "        cost[i][int(y_train[i])] = -1 * l_sum\n",
    "        Z += l_sum\n",
    "\n",
    "    block = BLOCKS[n]\n",
    "    ci = nn.Sequential(model[6], model[7], model[8])\n",
    "\n",
    "    if n < 17:\n",
    "        ci = nn.Sequential(BLOCKS[17], ci)\n",
    "\n",
    "    if n < 9:\n",
    "        ci = nn.Sequential(BLOCKS[9], ci)\n",
    "\n",
    "    tmp_m = nn.Sequential(block, ci, nn.Softmax(dim=0))\n",
    "    optimizer = torch.optim.Adam(tmp_m.parameters(), lr=L_RATE)\n",
    "    tries = 0\n",
    "    X_batch_t = torch.zeros(B_SIZE, FILTERS, 32, 32)\n",
    "\n",
    "    while gamma < GAMMA_T and ((CHECK_STEP * tries) < MAX_ITER):\n",
    "        train_acc = 0\n",
    "        test_acc = 0\n",
    "        err = 0\n",
    "\n",
    "        for batch in range(1, CHECK_STEP + 1):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ints = np.random.random_integers(np.shape(X_train)[0] - 1, size=B_SIZE)\n",
    "            X_batch = X_train[ints]\n",
    "            y_batch = Variable(torch.from_numpy(y_train[ints])).long()\n",
    "\n",
    "            if TR:\n",
    "                X_batch = _transform(X_batch)\n",
    "\n",
    "            data = Variable(torch.from_numpy(X_batch)).float()\n",
    "\n",
    "            for i in range(n):\n",
    "                data = BLOCKS[i](data)\n",
    "\n",
    "            output = tmp_m(data)\n",
    "            loss = torch.exp(criterion(output, y_batch))\n",
    "            loss.backward()\n",
    "            err += loss.data.item()\n",
    "\n",
    "            output = tmp_m(data)\n",
    "            train_acc += np.mean(torch.max(output, 1)[1].cpu().data.numpy() == y_train[ints])\n",
    "\n",
    "            model.eval()\n",
    "            ints = np.random.random_integers(np.shape(X_test)[0] - 1, size=(B_SIZE))\n",
    "            X_batch = X_test[ints]\n",
    "            data = Variable(torch.from_numpy(X_batch)).float()\n",
    "\n",
    "            for i in range(n):\n",
    "                data = BLOCKS[i](data)\n",
    "\n",
    "            output = tmp_m(data)\n",
    "            test_acc += np.mean(torch.max(output, 1)[1].cpu().data.numpy() == y_test[ints])\n",
    "            model.train()\n",
    "\n",
    "            if batch % PRINT_STEP == 0:\n",
    "                train_acc /= PRINT_STEP\n",
    "                test_acc /= PRINT_STEP\n",
    "                err /= PRINT_STEP\n",
    "                _write(['n', n, 'batch', iters + batch + (CHECK_STEP * tries), 'train_acc', train_acc, 'test_acc', test_acc])\n",
    "                train_acc = 0\n",
    "                test_acc = 0\n",
    "                err = 0\n",
    "\n",
    "            for p in tmp_m.parameters():\n",
    "                p.grad.data.clamp_(-.1, .1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        train_acc, out = _performance(tmp_m, X_train, y_train, n)\n",
    "        g_curr = -1 * np.sum(out * cost) / Z\n",
    "        gamma = (g_curr ** 2 - g_prev ** 2) / (1 - g_prev ** 2)\n",
    "\n",
    "        if gamma > 0:\n",
    "            gamma = np.sqrt(gamma)\n",
    "        else:\n",
    "            gamma = -1 * np.sqrt(-1 * gamma)\n",
    "\n",
    "        a_curr = 0.5 * np.log((1 + g_curr) / (1 - g_curr))\n",
    "\n",
    "        tries += 1\n",
    "\n",
    "        if gamma > GAMMA_T or ((CHECK_STEP * tries) >= MAX_ITER):\n",
    "            iters = iters + (tries * CHECK_STEP)\n",
    "            _write([gamma, g_curr, g_prev])\n",
    "            _write(['a_{t+1}:', a_curr, 'gamma:', gamma])\n",
    "\n",
    "    s += out * a_curr - Xoutput_previous * a_prev\n",
    "    test_acc, _ = _performance(tmp_m, X_test, y_test, n)\n",
    "    _write(['t', ROUNDS, 'n_batches:', tries * CHECK_STEP, 'test_acc:', test_acc])\n",
    "    g_prev = g_curr\n",
    "\n",
    "torch.save(model.state_dict(), modelPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
