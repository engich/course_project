{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torchfile\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBase(nn.Sequential):\n",
    "    def __init__(self, fn, *args):\n",
    "        super(LBase, self).__init__(*args)\n",
    "        self.lambda_func = fn\n",
    "\n",
    "    def forward_prepare(self, x):\n",
    "        output = []\n",
    "\n",
    "        for module in self._modules.values():\n",
    "            output.append(module(x))\n",
    "\n",
    "        return output if output else x\n",
    "\n",
    "\n",
    "class LForward(LBase):\n",
    "    def forward(self, x):\n",
    "        return self.lambda_func(self.forward_prepare(x))\n",
    "\n",
    "\n",
    "class LMap(LBase):\n",
    "    def forward(self, x):\n",
    "        return list(map(self.lambda_func, self.forward_prepare(x)))\n",
    "\n",
    "\n",
    "class LReduce(LBase):\n",
    "    def forward(self, x):\n",
    "        return reduce(self.lambda_func, self.forward_prepare(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.Sequential(\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(16, 16, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(16),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(16, 32, (3, 3), (2, 2), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 nn.Sequential(\n",
    "                          nn.AvgPool2d((1, 1), (2, 2)),\n",
    "                          LReduce(lambda x, y, dim=1: torch.cat((x, y), dim),\n",
    "                                  LForward(lambda x: x),\n",
    "                                  LForward(lambda x: x * 0),\n",
    "                                  ),\n",
    "                      ),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(32, 32, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(32),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(32, 64, (3, 3), (2, 2), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 nn.Sequential(\n",
    "                          nn.AvgPool2d((1, 1), (2, 2)),\n",
    "                          LReduce(lambda x, y, dim=1: torch.cat((x, y), dim),\n",
    "                                  LForward(lambda x: x),\n",
    "                                  LForward(lambda x: x * 0),\n",
    "                                  ),\n",
    "                      ),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            LMap(lambda x: x,\n",
    "                 nn.Sequential(\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), 1, 1, bias=False),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                      ),\n",
    "                 LForward(lambda x: x),\n",
    "                 ),\n",
    "            LReduce(lambda x, y: x + y),\n",
    "            nn.ReLU(),\n",
    "        ),\n",
    "    ),\n",
    "    nn.AvgPool2d((8, 8), (1, 1)),\n",
    "    LForward(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Sequential(LForward(lambda x: x.view(1, -1) if 1 == len(x.size()) else x), nn.Linear(64, 10)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA_F = 0.5\n",
    "CHECK_STEP = 10000\n",
    "D_PATH = 'MNIST.t7'\n",
    "modelPath = 'm.pt'\n",
    "GAMMA_T = 0.0001\n",
    "L_RATE = 0.001\n",
    "MAX_ITER = 10000\n",
    "TR = False\n",
    "PRINT_STEP = 100\n",
    "B_SIZE = 100\n",
    "FILTERS = 15\n",
    "ROUNDS = 25\n",
    "CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write(print_arr):\n",
    "    for v in print_arr: sys.stdout.write(str(v) + '\\t')\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def _transform(X):\n",
    "    tmp = np.zeros((np.shape(X)[0], 3, 38, 38))\n",
    "    tmp[:, :, 2:34, 2:34] = X\n",
    "\n",
    "    for i in range(np.shape(X)[0]):\n",
    "        r1 = np.random.randint(4)\n",
    "        r2 = np.random.randint(4)\n",
    "        X[i] = tmp[i, :, r1: r1 + 32, r2: r2 + 32]\n",
    "\n",
    "        if np.random.uniform() > .5:\n",
    "            X[i] = X[i, :, :, ::-1]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def _performance(net, X, y, n):\n",
    "    acc = 0.\n",
    "    model.eval()\n",
    "    out = np.zeros((X.shape[0], 10))\n",
    "\n",
    "    for batch in range(int(X.shape[0] / B_SIZE)):\n",
    "        start = batch * B_SIZE\n",
    "        stop = (batch + 1) * B_SIZE - 1\n",
    "        ints = np.linspace(start, stop, B_SIZE).astype(int)\n",
    "        data = Variable(torch.from_numpy(X[ints])).float().cuda()\n",
    "\n",
    "        for i in range(n):\n",
    "            data = BLOCKS[i](data)\n",
    "\n",
    "        output = net(data)\n",
    "        acc += np.mean(torch.max(output, 1)[1].cpu().data.numpy() == y[ints])\n",
    "        out[ints] = output.cpu().data.numpy()\n",
    "\n",
    "    acc /= (X.shape[0] / B_SIZE)\n",
    "    model.train()\n",
    "    return acc, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKS = {0: nn.Sequential(model[0], model[1], model[2])}\n",
    "\n",
    "for i in range(8):\n",
    "    BLOCKS[1 + i] = model[3][i]\n",
    "\n",
    "for i in range(8):\n",
    "    BLOCKS[9 + i] = model[4][i]\n",
    "\n",
    "for i in range(8):\n",
    "    BLOCKS[17 + i] = model[5][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchfile.load(D_PATH)\n",
    "\n",
    "X_train = data.Xtrain\n",
    "y_train = data.Ytrain - 1\n",
    "X_test = data.Xtest\n",
    "y_test = data.Ytest - 1\n",
    "\n",
    "cut = int(np.shape(X_train)[0] / B_SIZE * B_SIZE)\n",
    "\n",
    "train_size = cut\n",
    "X_train = X_train[:cut]\n",
    "y_train = y_train[:cut]\n",
    "\n",
    "cut = int(np.shape(X_test)[0] / B_SIZE * B_SIZE)\n",
    "\n",
    "test_size = cut\n",
    "X_test = X_test[:test_size]\n",
    "y_test = y_test[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "a_prev = 0.0\n",
    "a_curr = -1.0\n",
    "s = np.zeros((train_size, CLASSES))\n",
    "cost = np.zeros((train_size, CLASSES))\n",
    "Xoutput_previous = np.zeros((train_size, CLASSES))\n",
    "y_batch = np.zeros((B_SIZE))\n",
    "y_batch_t = np.zeros((B_SIZE))\n",
    "g_curr = 0\n",
    "g_prev = GAMMA_F\n",
    "iters = 0\n",
    "tries = 0\n",
    "\n",
    "for n in range(ROUNDS):\n",
    "    gamma = -1\n",
    "    Z = 0\n",
    "\n",
    "    for i in range(train_size):\n",
    "        l_sum = 0\n",
    "\n",
    "        for l in range(CLASSES):\n",
    "            if l != y_train[i]:\n",
    "                cost[i][l] = np.exp(s[i][l] - s[i][int(y_train[i])])\n",
    "                l_sum += cost[i][l]\n",
    "\n",
    "        cost[i][int(y_train[i])] = -1 * l_sum\n",
    "        Z += l_sum\n",
    "\n",
    "    block = BLOCKS[n]\n",
    "    ci = nn.Sequential(model[6], model[7], model[8])\n",
    "\n",
    "    if n < 17:\n",
    "        ci = nn.Sequential(BLOCKS[17], ci)\n",
    "\n",
    "    if n < 9:\n",
    "        ci = nn.Sequential(BLOCKS[9], ci)\n",
    "\n",
    "    tmp_m = nn.Sequential(block, ci, nn.Softmax(dim=0))\n",
    "    tmp_m = tmp_m.cuda()\n",
    "    optimizer = torch.optim.Adam(tmp_m.parameters(), lr=L_RATE)\n",
    "    tries = 0\n",
    "    X_batch_t = torch.zeros(B_SIZE, FILTERS, 32, 32)\n",
    "\n",
    "    while gamma < GAMMA_T and ((CHECK_STEP * tries) < MAX_ITER):\n",
    "        train_acc = 0\n",
    "        test_acc = 0\n",
    "        err = 0\n",
    "\n",
    "        for batch in range(1, CHECK_STEP + 1):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ints = np.random.random_integers(np.shape(X_train)[0] - 1, size=B_SIZE)\n",
    "            X_batch = X_train[ints]\n",
    "            y_batch = Variable(torch.from_numpy(y_train[ints])).cuda().long()\n",
    "\n",
    "            if TR:\n",
    "                X_batch = _transform(X_batch)\n",
    "\n",
    "            data = Variable(torch.from_numpy(X_batch)).float().cuda()\n",
    "\n",
    "            for i in range(n):\n",
    "                data = BLOCKS[i](data)\n",
    "\n",
    "            output = tmp_m(data)\n",
    "            loss = torch.exp(criterion(output, y_batch))\n",
    "            loss.backward()\n",
    "            err += loss.data[0]\n",
    "\n",
    "            output = tmp_m(data)\n",
    "            train_acc += np.mean(torch.max(output, 1)[1].cpu().data.numpy() == y_train[ints])\n",
    "\n",
    "            model.eval()\n",
    "            ints = np.random.random_integers(np.shape(X_test)[0] - 1, size=(B_SIZE))\n",
    "            X_batch = X_test[ints]\n",
    "            data = Variable(torch.from_numpy(X_batch)).float().cuda()\n",
    "\n",
    "            for i in range(n):\n",
    "                data = BLOCKS[i](data)\n",
    "\n",
    "            output = tmp_m(data)\n",
    "            test_acc += np.mean(torch.max(output, 1)[1].cpu().data.numpy() == y_test[ints])\n",
    "            model.train()\n",
    "\n",
    "            if batch % PRINT_STEP == 0:\n",
    "                train_acc /= PRINT_STEP\n",
    "                test_acc /= PRINT_STEP\n",
    "                err /= PRINT_STEP\n",
    "                _write([n, ROUNDS, iters + batch + (CHECK_STEP * tries), err, train_acc, test_acc])\n",
    "                train_acc = 0\n",
    "                test_acc = 0\n",
    "                err = 0\n",
    "\n",
    "            for p in tmp_m.parameters():\n",
    "                p.grad.data.clamp_(-.1, .1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        train_acc, out = _performance(tmp_m, X_train, y_train, n)\n",
    "        g_curr = -1 * np.sum(out * cost) / Z\n",
    "        gamma = (g_curr ** 2 - g_prev ** 2) / (1 - g_prev ** 2)\n",
    "\n",
    "        if gamma > 0:\n",
    "            gamma = np.sqrt(gamma)\n",
    "        else:\n",
    "            gamma = -1 * np.sqrt(-1 * gamma)\n",
    "\n",
    "        a_curr = 0.5 * np.log((1 + g_curr) / (1 - g_curr))\n",
    "\n",
    "        tries += 1\n",
    "\n",
    "        if gamma > GAMMA_T or ((CHECK_STEP * tries) >= MAX_ITER):\n",
    "            iters = iters + (tries * CHECK_STEP)\n",
    "            _write([gamma, g_curr, g_prev])\n",
    "            _write(['a_{t+1}:', a_curr, 'gamma:', gamma])\n",
    "\n",
    "    s += out * a_curr - Xoutput_previous * a_prev\n",
    "    test_acc, _ = _performance(tmp_m, X_test, y_test, n)\n",
    "    _write(['t', ROUNDS, 'n_batches:', tries * CHECK_STEP, 'test test_acc:', test_acc])\n",
    "    g_prev = g_curr\n",
    "\n",
    "torch.save(model.state_dict(), modelPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
